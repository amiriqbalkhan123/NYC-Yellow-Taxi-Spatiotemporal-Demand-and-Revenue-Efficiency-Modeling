{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71ed96ae-b610-49c3-8049-a3aadf6eaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75d86445-3f87-42e6-b92f-97cc5028e166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: ['apr_2025_cleaned.parquet', 'aug_2025_cleaned.parquet', 'dec_2024_cleaned.parquet', 'feb_2025_cleaned.parquet', 'jan_2025_cleaned.parquet', 'jul_2025_cleaned.parquet', 'jun_2025_cleaned.parquet', 'mar_2025_cleaned.parquet', 'may_2025_cleaned.parquet', 'nov_2025_cleaned.parquet', 'oct_2025_cleaned.parquet', 'sep_2025_cleaned.parquet']\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in os.listdir('.') if f.endswith('.parquet')]\n",
    "files.sort()\n",
    "\n",
    "print('Files to process:', files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ecfb9ba8-14f4-4464-a1dc-da1f0cc6e5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing apr_2025_cleaned.parquet ...\n",
      "Processing aug_2025_cleaned.parquet ...\n",
      "Processing dec_2024_cleaned.parquet ...\n",
      "Processing feb_2025_cleaned.parquet ...\n",
      "Processing jan_2025_cleaned.parquet ...\n",
      "Processing jul_2025_cleaned.parquet ...\n",
      "Processing jun_2025_cleaned.parquet ...\n",
      "Processing mar_2025_cleaned.parquet ...\n",
      "Processing may_2025_cleaned.parquet ...\n",
      "Processing nov_2025_cleaned.parquet ...\n",
      "Processing oct_2025_cleaned.parquet ...\n",
      "Processing sep_2025_cleaned.parquet ...\n",
      "Concatenating all months....\n",
      "Final Parquet saved as : nyc_taxi_jan_2025_nov_2025.parquet\n"
     ]
    }
   ],
   "source": [
    "columns_to_load = [\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pu_location_id\",\n",
    "    \"do_location_id\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"total_amount\",\n",
    "    \"airport_fee\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    print(f'Processing {f} ...')\n",
    "\n",
    "    temp = pd.read_parquet(f, columns=columns_to_load)\n",
    "    temp['pickup_datetime'] = pd.to_datetime(temp['pickup_datetime'])\n",
    "    temp['dropoff_datetime'] = pd.to_datetime(temp['dropoff_datetime'])\n",
    "    \n",
    "    temp[\"trip_distance\"] = temp[\"trip_distance\"].astype(\"float32\")\n",
    "    temp[\"fare_amount\"] = temp[\"fare_amount\"].astype(\"float32\")\n",
    "    temp[\"extra\"] = temp[\"extra\"].astype(\"float32\")\n",
    "    temp[\"mta_tax\"] = temp[\"mta_tax\"].astype(\"float32\")\n",
    "    temp[\"tip_amount\"] = temp[\"tip_amount\"].astype(\"float32\")\n",
    "    temp[\"tolls_amount\"] = temp[\"tolls_amount\"].astype(\"float32\")\n",
    "    temp[\"total_amount\"] = temp[\"total_amount\"].astype(\"float32\")\n",
    "    temp[\"passenger_count\"] = temp[\"passenger_count\"].astype(\"int8\")\n",
    "    temp[\"airport_fee\"] = temp[\"airport_fee\"].astype(\"float32\")\n",
    "\n",
    "    # Derive additional columns useful for EDA/statistics\n",
    "    temp['trip_duration'] = (temp['dropoff_datetime'] - temp['pickup_datetime']).dt.total_seconds() / 60\n",
    "    temp['year'] = temp['pickup_datetime'].dt.year\n",
    "    temp['month'] = temp['pickup_datetime'].dt.month\n",
    "    temp['day_of_week'] = temp['pickup_datetime'].dt.dayofweek\n",
    "    temp['hour'] = temp['pickup_datetime'].dt.hour\n",
    "    temp['is_weekend'] = temp['day_of_week'].isin([5,6])\n",
    "\n",
    "    dfs.append(temp)\n",
    "    del temp\n",
    "\n",
    "print('Concatenating all months....')\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs\n",
    "\n",
    "output_file = 'nyc_taxi_jan_2025_nov_2025.parquet'\n",
    "df.to_parquet(output_file, engine='pyarrow', index=False,compression='snappy')\n",
    "print(f'Final Parquet saved as : {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19d99ee7-a9e4-49d5-aa33-8ac98f1d537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all datasets merged into one!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eab5c5-bb7d-48d7-89d0-56964cfb33d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
